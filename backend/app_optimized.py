from flask import Flask, jsonify, request
from flask_cors import CORS
import yfinance as yf
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
from scipy.stats import pearsonr
import redis
import json
import threading
from functools import wraps
from concurrent.futures import ThreadPoolExecutor, as_completed
import gzip
import io
import time
import os
from typing import List, Dict, Optional, Tuple

import data_storage  # å°å…¥æœ¬åœ°æ•¸æ“šå­˜å„²æ¨¡çµ„
app = Flask(__name__)
CORS(app)

# æ•¸æ“šæ›´æ–°æ¨™èªŒï¼ˆç¢ºä¿åªæ›´æ–°ä¸€æ¬¡ï¼‰
DATA_UPDATE_DONE = False
DATA_UPDATE_LOCK = threading.Lock()

def ensure_data_updated():
    """ç¢ºä¿æ•¸æ“šå·²æ›´æ–°åˆ°æœ€æ–°ï¼ˆç·šç¨‹å®‰å…¨ï¼ŒåªåŸ·è¡Œä¸€æ¬¡ï¼‰"""
    global DATA_UPDATE_DONE
    
    with DATA_UPDATE_LOCK:
        if DATA_UPDATE_DONE:
            return
        
        try:
            print("\n" + "=" * 60)
            print("ğŸ”„ æ­£åœ¨æª¢æŸ¥ä¸¦æ›´æ–°æ•¸æ“šåˆ°æœ€æ–°...")
            print("=" * 60)
            
            # ç²å–æ‰€æœ‰é‚£æ–¯é”å…‹è‚¡ç¥¨ä»£ç¢¼
            nasdaq_tickers = data_storage.get_nasdaq_tickers()
            
            # ç²å–çµ±è¨ˆè³‡è¨Š
            stats = data_storage.get_storage_stats()
            
            if stats.get('total_stocks', 0) > 0:
                print(f"ğŸ“Š æœ¬åœ°å·²æœ‰ {stats['total_stocks']} æ”¯è‚¡ç¥¨æ•¸æ“š")
                print("â© åŸ·è¡Œå¢é‡æ›´æ–°ï¼Œåªä¸‹è¼‰æœ€æ–°æ•¸æ“š...")
                
                # åŸ·è¡Œå¢é‡æ›´æ–°
                result = data_storage.bulk_update_incremental(
                    symbols=nasdaq_tickers,
                    end_date=None  # None è¡¨ç¤ºæ›´æ–°åˆ°ä»Šå¤©
                )
                
                updated = result.get('updated', 0)
                skipped = result.get('skipped', 0)
                print(f"âœ… æ›´æ–°å®Œæˆï¼æ›´æ–°äº† {updated} æ”¯è‚¡ç¥¨ï¼Œè·³é {skipped} æ”¯")
            else:
                print("ğŸ“¥ æœ¬åœ°ç„¡æ•¸æ“šï¼Œå°‡ä¸‹è¼‰æ‰€æœ‰æ­·å²æ•¸æ“š...")
                print("â³ é€™å¯èƒ½éœ€è¦å¹¾åˆ†é˜ï¼Œè«‹ç¨å€™...")
                
                # åŸ·è¡Œå®Œæ•´ä¸‹è¼‰
                result = data_storage.bulk_download_to_local(
                    symbols=nasdaq_tickers
                )
                
                print(f"âœ… ä¸‹è¼‰å®Œæˆï¼å…± {result.get('successful', 0)} æ”¯è‚¡ç¥¨")
            
            DATA_UPDATE_DONE = True
            print("=" * 60 + "\n")
            
        except Exception as e:
            print(f"âš ï¸  æ•¸æ“šæ›´æ–°è­¦å‘Š: {str(e)}")
            print("ğŸ”§ API å°‡ä½¿ç”¨ç¾æœ‰æ•¸æ“šç¹¼çºŒé‹è¡Œ")
            print("=" * 60 + "\n")
            DATA_UPDATE_DONE = True  # å³ä½¿å¤±æ•—ä¹Ÿæ¨™è¨˜ç‚ºå®Œæˆï¼Œé¿å…é‡è¤‡å˜—è©¦

# Redis é…ç½®
try:
    redis_client = redis.Redis(
        host='redis',
        port=6379,
        db=0,
        decode_responses=False,
        socket_connect_timeout=5
    )
    redis_client.ping()
    REDIS_AVAILABLE = True
    print("âœ“ Redis é€£æ¥æˆåŠŸ")
except:
    REDIS_AVAILABLE = False
    print("âœ— Redis ä¸å¯ç”¨ï¼Œä½¿ç”¨ç„¡ç·©å­˜æ¨¡å¼")

# ä¸‰å¤§æŒ‡æ•¸é…ç½®
INDICES = {
    '^IXIC': {
        'name': 'NASDAQ',
        'constituents': ['AAPL', 'MSFT', 'GOOGL', 'AMZN', 'NVDA', 'META', 'TSLA', 'AVGO', 'COST', 'NFLX']
    },
    '^DJI': {
        'name': 'é“ç“Šå·¥æ¥­æŒ‡æ•¸',
        'constituents': ['AAPL', 'MSFT', 'UNH', 'GS', 'HD', 'MCD', 'CAT', 'V', 'AMGN', 'BA']
    },
    '^GSPC': {
        'name': 'S&P 500',
        'constituents': ['AAPL', 'MSFT', 'GOOGL', 'AMZN', 'NVDA', 'META', 'BRK-B', 'TSLA', 'V', 'UNH']
    }
}

# æŒ‡æ•¸å°æ‡‰çš„è‚¡ç¥¨æ•¸æ“šç›®éŒ„
INDEX_DATA_DIRS = {
    '^IXIC': '/app/data/stocks',          # NASDAQ è‚¡ç¥¨æ•¸æ“šç›®éŒ„
    '^DJI': '/app/data/dow_jones_stocks', # é“ç“Šå·¥æ¥­æŒ‡æ•¸è‚¡ç¥¨æ•¸æ“šç›®éŒ„
    '^GSPC': '/app/data/sp500_stocks'     # S&P 500 è‚¡ç¥¨æ•¸æ“šç›®éŒ„
}

# ç·©å­˜æ™‚é–“è¨­ç½®ï¼ˆç§’ï¼‰
CACHE_TTL_STOCK_DATA = 3600  # è‚¡ç¥¨æ•¸æ“šç·©å­˜ 1 å°æ™‚
CACHE_TTL_CORRELATION = 7200  # ç›¸é—œæ€§æ•¸æ“šç·©å­˜ 2 å°æ™‚
CACHE_TTL_TICKER_LIST = 86400 * 7  # è‚¡ç¥¨åˆ—è¡¨ç·©å­˜ 7 å¤©
CACHE_TTL_FULL_CORRELATION = 3600  # å…¨å¸‚å ´ç›¸é—œæ€§ç·©å­˜ 1 å°æ™‚

def get_cache_key(prefix, *args):
    """ç”Ÿæˆç·©å­˜éµ"""
    return f"{prefix}:{':'.join(str(arg) for arg in args)}"

def cache_result(ttl=3600):
    """ç·©å­˜è£é£¾å™¨"""
    def decorator(func):
        @wraps(func)
        def wrapper(*args, **kwargs):
            if not REDIS_AVAILABLE:
                return func(*args, **kwargs)
            
            # ç”Ÿæˆç·©å­˜éµ
            cache_key = get_cache_key(func.__name__, *args, *sorted(kwargs.items()))
            
            try:
                # å˜—è©¦å¾ç·©å­˜ç²å–
                cached = redis_client.get(cache_key)
                if cached:
                    print(f"âœ“ ç·©å­˜å‘½ä¸­: {cache_key}")
                    return json.loads(gzip.decompress(cached))
            except Exception as e:
                print(f"ç·©å­˜è®€å–éŒ¯èª¤: {e}")
            
            # åŸ·è¡Œå‡½æ•¸
            result = func(*args, **kwargs)
            
            # ä¿å­˜åˆ°ç·©å­˜
            try:
                if result is not None:
                    compressed = gzip.compress(json.dumps(result).encode())
                    redis_client.setex(cache_key, ttl, compressed)
                    print(f"âœ“ ç·©å­˜ä¿å­˜: {cache_key}")
            except Exception as e:
                print(f"ç·©å­˜ä¿å­˜éŒ¯èª¤: {e}")
            
            return result
        return wrapper
    return decorator

@cache_result(ttl=CACHE_TTL_STOCK_DATA)
def download_stock_data(symbol, start_date='2010-01-01', end_date=None):
    """å¾ Yahoo Finance ä¸‹è¼‰è‚¡ç¥¨æ­·å²æ•¸æ“šï¼ˆå¸¶ç·©å­˜ï¼‰"""
    try:
        if end_date is None:
            end_date = datetime.now().strftime('%Y-%m-%d')
        
        print(f"ä¸‹è¼‰ {symbol} æ•¸æ“š: {start_date} è‡³ {end_date}")
        ticker = yf.Ticker(symbol)
        hist = ticker.history(start=start_date, end=end_date)
        
        if hist.empty:
            print(f"è­¦å‘Š: {symbol} ç„¡æ•¸æ“š")
            return None
        
        print(f"æˆåŠŸä¸‹è¼‰ {symbol} {len(hist)} ç­†æ•¸æ“š")
        
        # è½‰æ›ç‚ºæ‰€éœ€æ ¼å¼ï¼ˆå„ªåŒ–ç‰ˆï¼šä½¿ç”¨å‘é‡åŒ–æ“ä½œï¼‰
        data = {
            'date': hist.index.strftime('%Y-%m-%d').tolist(),
            'open': hist['Open'].astype(float).tolist(),
            'high': hist['High'].astype(float).tolist(),
            'low': hist['Low'].astype(float).tolist(),
            'close': hist['Close'].astype(float).tolist(),
            'volume': hist['Volume'].astype(int).tolist()
        }
        
        # è½‰æ›ç‚ºåˆ—è¡¨æ ¼å¼
        result = [
            {
                'date': data['date'][i],
                'open': data['open'][i],
                'high': data['high'][i],
                'low': data['low'][i],
                'close': data['close'][i],
                'volume': data['volume'][i]
            }
            for i in range(len(data['date']))
        ]
        
        return result
    except Exception as e:
        print(f"ä¸‹è¼‰ {symbol} æ•¸æ“šå¤±æ•—: {str(e)}")
        return None

def calculate_correlation(index_data, stock_data):
    """è¨ˆç®—è‚¡ç¥¨èˆ‡æŒ‡æ•¸çš„ç›¸é—œä¿‚æ•¸ï¼ˆå„ªåŒ–ç‰ˆï¼‰"""
    try:
        # ä½¿ç”¨å­—å…¸æŸ¥æ‰¾å„ªåŒ–æ—¥æœŸå°é½Š
        stock_dict = {item['date']: item['close'] for item in stock_data}
        
        index_closes = []
        stock_closes = []
        
        for item in index_data:
            date = item['date']
            if date in stock_dict:
                index_closes.append(item['close'])
                stock_closes.append(stock_dict[date])
        
        if len(index_closes) < 30:
            print(f"è­¦å‘Š: æ•¸æ“šé»ä¸è¶³ ({len(index_closes)} é»)")
            return 0.0
        
        print(f"è¨ˆç®—ç›¸é—œæ€§: ä½¿ç”¨ {len(index_closes)} å€‹æ•¸æ“šé»")
        
        # è¨ˆç®—çš®çˆ¾æ£®ç›¸é—œä¿‚æ•¸
        correlation, p_value = pearsonr(index_closes, stock_closes)
        
        print(f"ç›¸é—œä¿‚æ•¸: {correlation:.4f}, på€¼: {p_value:.6f}")
        
        return float(correlation)
    except Exception as e:
        print(f"è¨ˆç®—ç›¸é—œæ€§å¤±æ•—: {str(e)}")
        return 0.0

def download_stock_info(symbol):
    """ä¸‹è¼‰è‚¡ç¥¨è³‡è¨Š"""
    try:
        ticker = yf.Ticker(symbol)
        return ticker.info.get('longName', symbol)
    except:
        return symbol

# åœ¨ç¬¬ä¸€å€‹è«‹æ±‚å‰ç¢ºä¿æ•¸æ“šå·²æ›´æ–°
@app.before_request
def before_first_request():
    """åœ¨ç¬¬ä¸€å€‹è«‹æ±‚å‰åŸ·è¡Œæ•¸æ“šæ›´æ–°"""
    ensure_data_updated()

@app.route('/index/<symbol>', methods=['GET'])
def get_index_data(symbol):
    """ç²å–æŒ‡æ•¸æ­·å²æ•¸æ“šï¼ˆæ”¯æŒè‡ªå®šç¾©æ—¥æœŸç¯„åœï¼Œå„ªå…ˆå¾æœ¬åœ°è®€å–ï¼‰"""
    if symbol not in INDICES:
        return jsonify({'error': 'ç„¡æ•ˆçš„æŒ‡æ•¸ä»£ç¢¼'}), 400
    
    # å¾æŸ¥è©¢åƒæ•¸ç²å–æ—¥æœŸç¯„åœ
    start_date = request.args.get('start_date', '2010-01-01')
    end_date = request.args.get('end_date', None)
    
    print(f"\n{'='*50}")
    print(f"API è«‹æ±‚: ç²å– {INDICES[symbol]['name']} æ­·å²æ•¸æ“š")
    print(f"æ—¥æœŸç¯„åœ: {start_date} è‡³ {end_date or 'ä»Šå¤©'}")
    print(f"{'='*50}")
    
    # å„ªå…ˆå¾æœ¬åœ°æª”æ¡ˆè®€å–
    print(f"å˜—è©¦å¾æœ¬åœ°æª”æ¡ˆè®€å– {symbol} ...")
    local_data = data_storage.load_stock_data(symbol)
    print(f"æœ¬åœ°æª”æ¡ˆè®€å–çµæœ: {local_data is not None}")
    
    if local_data:
        print(f"æœ¬åœ°æ•¸æ“šéµ: {list(local_data.keys())}")
        print(f"dates æ¬„ä½: {local_data.get('dates') is not None}, {len(local_data.get('dates', [])) if local_data.get('dates') else 0} ç­†")
        print(f"close æ¬„ä½: {local_data.get('close') is not None}, {len(local_data.get('close', [])) if local_data.get('close') else 0} ç­†")
    
    if local_data and local_data.get('dates') and local_data.get('close'):
        # æ ¹æ“šæ—¥æœŸç¯„åœéæ¿¾æ•¸æ“š
        dates = local_data['dates']
        close_prices = local_data['close']
        
        # æ‰¾åˆ°æ—¥æœŸç¯„åœå…§çš„ç´¢å¼•
        start_idx = 0
        end_idx = len(dates)
        
        for i, date in enumerate(dates):
            if date >= start_date:
                start_idx = i
                break
        
        if end_date:
            for i in range(len(dates) - 1, -1, -1):
                if dates[i] <= end_date:
                    end_idx = i + 1
                    break
        
        # éæ¿¾å¾Œçš„æ•¸æ“š
        filtered_dates = dates[start_idx:end_idx]
        filtered_closes = close_prices[start_idx:end_idx]
        
        if filtered_dates:
            # è½‰æ›ç‚º API æ ¼å¼
            data = [
                {
                    'date': filtered_dates[i],
                    'close': filtered_closes[i],
                    # é€™äº›æ¬„ä½åœ¨æœ¬åœ°æ•¸æ“šä¸­å¯èƒ½ä¸å­˜åœ¨ï¼Œä½¿ç”¨ close ä½œç‚ºæ›¿ä»£
                    'open': filtered_closes[i],
                    'high': filtered_closes[i],
                    'low': filtered_closes[i],
                    'volume': 0
                }
                for i in range(len(filtered_dates))
            ]
            
            print(f"âœ“ å¾æœ¬åœ°æª”æ¡ˆè®€å– {len(data)} ç­†æ•¸æ“š")
            print(f"æ•¸æ“šç¯„åœ: {data[0]['date']} è‡³ {data[-1]['date']}")
            
            return jsonify({
                'symbol': symbol,
                'name': INDICES[symbol]['name'],
                'history': data,
                'data_range': {
                    'start': data[0]['date'],
                    'end': data[-1]['date'],
                    'count': len(data)
                }
            })
    
    # å¦‚æœæœ¬åœ°æ²’æœ‰æ•¸æ“šï¼Œå›é€€åˆ°ä¸‹è¼‰
    print("âš ï¸  æœ¬åœ°ç„¡æ•¸æ“šï¼Œå¾ Yahoo Finance ä¸‹è¼‰...")
    data = download_stock_data(symbol, start_date=start_date, end_date=end_date)
    
    if data is None or len(data) == 0:
        return jsonify({'error': 'ç„¡æ³•ç²å–æ•¸æ“š'}), 500
    
    print(f"è¿”å› {len(data)} ç­†æ•¸æ“š")
    print(f"æ•¸æ“šç¯„åœ: {data[0]['date']} è‡³ {data[-1]['date']}")
    
    return jsonify({
        'symbol': symbol,
        'name': INDICES[symbol]['name'],
        'history': data,
        'data_range': {
            'start': data[0]['date'],
            'end': data[-1]['date'],
            'count': len(data)
        }
    })

@app.route('/correlation/<symbol>', methods=['GET'])
@cache_result(ttl=CACHE_TTL_CORRELATION)
def get_correlation_data(symbol):
    """ç²å–æŒ‡æ•¸æˆåˆ†è‚¡èˆ‡æŒ‡æ•¸çš„ç›¸é—œæ€§ï¼ˆå„ªåŒ–ç‰ˆï¼šä¸¦è¡Œä¸‹è¼‰ï¼‰"""
    if symbol not in INDICES:
        return jsonify({'error': 'ç„¡æ•ˆçš„æŒ‡æ•¸ä»£ç¢¼'}), 400
    
    print(f"\n{'='*50}")
    print(f"API è«‹æ±‚: è¨ˆç®— {INDICES[symbol]['name']} ç›¸é—œæ€§")
    print(f"{'='*50}")
    
    # ä¸‹è¼‰æŒ‡æ•¸æ•¸æ“š
    index_data = download_stock_data(symbol)
    if index_data is None or len(index_data) == 0:
        return jsonify({'error': 'ç„¡æ³•ç²å–æŒ‡æ•¸æ•¸æ“š'}), 500
    
    constituents = INDICES[symbol]['constituents']
    results = []
    
    print(f"\né–‹å§‹ä¸¦è¡Œä¸‹è¼‰ {len(constituents)} å€‹æˆåˆ†è‚¡æ•¸æ“š...")
    
    # ä½¿ç”¨ç·šç¨‹æ± ä¸¦è¡Œä¸‹è¼‰æ•¸æ“š
    with ThreadPoolExecutor(max_workers=5) as executor:
        # æäº¤æ‰€æœ‰ä¸‹è¼‰ä»»å‹™
        future_to_symbol = {
            executor.submit(download_stock_data, stock_symbol): stock_symbol 
            for stock_symbol in constituents
        }
        
        # åŒæ™‚ä¸‹è¼‰è‚¡ç¥¨åç¨±
        future_to_info = {
            executor.submit(download_stock_info, stock_symbol): stock_symbol 
            for stock_symbol in constituents
        }
        
        stock_data_map = {}
        stock_name_map = {}
        
        # æ”¶é›†è‚¡ç¥¨æ•¸æ“š
        for future in as_completed(future_to_symbol):
            stock_symbol = future_to_symbol[future]
            try:
                data = future.result()
                if data:
                    stock_data_map[stock_symbol] = data
                    print(f"âœ“ ä¸‹è¼‰å®Œæˆ: {stock_symbol}")
            except Exception as e:
                print(f"âœ— ä¸‹è¼‰å¤±æ•—: {stock_symbol} - {e}")
        
        # æ”¶é›†è‚¡ç¥¨åç¨±
        for future in as_completed(future_to_info):
            stock_symbol = future_to_info[future]
            try:
                name = future.result()
                stock_name_map[stock_symbol] = name
            except:
                stock_name_map[stock_symbol] = stock_symbol
    
    print(f"\nè¨ˆç®—ç›¸é—œæ€§...")
    
    # è¨ˆç®—ç›¸é—œæ€§
    for stock_symbol in constituents:
        if stock_symbol not in stock_data_map:
            continue
        
        stock_data = stock_data_map[stock_symbol]
        correlation = calculate_correlation(index_data, stock_data)
        
        results.append({
            'symbol': stock_symbol,
            'name': stock_name_map.get(stock_symbol, stock_symbol),
            'correlation': correlation
        })
        
        print(f"å®Œæˆ {stock_symbol}: ç›¸é—œæ€§ = {correlation:.4f}")
    
    # æŒ‰ç›¸é—œæ€§çµ•å°å€¼æ’åº
    results.sort(key=lambda x: abs(x['correlation']), reverse=True)
    
    print(f"\n{'='*50}")
    print(f"ç›¸é—œæ€§è¨ˆç®—å®Œæˆï¼å…± {len(results)} å€‹æˆåˆ†è‚¡")
    print(f"{'='*50}\n")
    
    return results

@cache_result(ttl=CACHE_TTL_TICKER_LIST)
def get_nasdaq_tickers():
    """ç²å–æ‰€æœ‰é‚£æ–¯é”å…‹è‚¡ç¥¨ä»£ç¢¼"""
    print("é–‹å§‹ä¸‹è¼‰é‚£æ–¯é”å…‹è‚¡ç¥¨åˆ—è¡¨...")
    try:
        # å¾ NASDAQ å®˜æ–¹ FTP ä¸‹è¼‰è‚¡ç¥¨åˆ—è¡¨
        url = "ftp://ftp.nasdaqtrader.com/symboldirectory/nasdaqlisted.txt"
        
        try:
            df = pd.read_csv(url, sep='|')
            # éæ¿¾æ‰æ¸¬è©¦è‚¡ç¥¨å’Œ ETF
            df = df[df['Test Issue'] == 'N']
            df = df[df['ETF'] == 'N']
            
            tickers = df['Symbol'].tolist()
            
            # ç§»é™¤æœ€å¾Œä¸€è¡Œï¼ˆé€šå¸¸æ˜¯æ–‡ä»¶å‰µå»ºæ—¥æœŸï¼‰
            if tickers and not tickers[-1].replace('.', '').replace('-', '').isalnum():
                tickers = tickers[:-1]
            
            print(f"âœ“ æˆåŠŸç²å– {len(tickers)} æ”¯é‚£æ–¯é”å…‹è‚¡ç¥¨")
            return tickers
        except Exception as e:
            print(f"å¾ NASDAQ FTP ä¸‹è¼‰å¤±æ•—: {e}")
            
            # å‚™ç”¨æ–¹æ¡ˆï¼šä½¿ç”¨æ“´å±•çš„ä¸»è¦è‚¡ç¥¨åˆ—è¡¨
            major_tickers = [
                'AAPL', 'MSFT', 'GOOGL', 'GOOG', 'AMZN', 'NVDA', 'META', 'TSLA',
                'AVGO', 'COST', 'NFLX', 'AMD', 'PEP', 'CSCO', 'ADBE', 'CMCSA',
                'INTC', 'TXN', 'QCOM', 'AMGN', 'INTU', 'AMAT', 'HON', 'ISRG',
                'BKNG', 'VRTX', 'ADP', 'GILD', 'SBUX', 'REGN', 'MU', 'ADI',
                'LRCX', 'PYPL', 'MDLZ', 'PANW', 'MELI', 'KLAC', 'SNPS', 'CDNS',
                'MAR', 'ABNB', 'CTAS', 'CRWD', 'CSX', 'NXPI', 'ORLY', 'MRVL',
                'ASML', 'FTNT', 'ADSK', 'MNST', 'DASH', 'WDAY', 'AEP', 'PCAR',
                'CHTR', 'CPRT', 'ROST', 'PAYX', 'ODFL', 'KDP', 'FAST', 'KHC',
                'CTSH', 'EA', 'DXCM', 'VRSK', 'GEHC', 'BKR', 'LULU', 'IDXX',
                'XEL', 'EXC', 'MCHP', 'CCEP', 'CSGP', 'TEAM', 'ZS', 'TTWO',
                'ANSS', 'ON', 'CDW', 'BIIB', 'ILMN', 'GFS', 'WBD', 'FANG',
                'DDOG', 'MDB', 'ZM', 'MRNA', 'ENPH', 'ALGN', 'RIVN', 'LCID',
                'COIN', 'ROKU', 'ZI', 'PINS', 'DOCU', 'SNOW', 'NET', 'CRWD',
                'OKTA', 'SHOP', 'SQ', 'UBER', 'LYFT', 'ABNB', 'SPOT', 'RBLX'
            ]
            print(f"ä½¿ç”¨å‚™ç”¨åˆ—è¡¨: {len(major_tickers)} æ”¯ä¸»è¦è‚¡ç¥¨")
            return major_tickers
            
    except Exception as e:
        print(f"ç²å–è‚¡ç¥¨åˆ—è¡¨éŒ¯èª¤: {e}")
        return []

@cache_result(ttl=CACHE_TTL_STOCK_DATA)
def download_stock_close_only(symbol, start_date='2020-01-01', end_date=None, retry_count=3):
    """ä¸‹è¼‰å–®æ”¯è‚¡ç¥¨çš„æ”¶ç›¤åƒ¹ï¼ˆåƒ…ç”¨æ–¼ç›¸é—œæ€§è¨ˆç®—ï¼Œå¸¶é‡è©¦æ©Ÿåˆ¶ï¼‰"""
    if end_date is None:
        end_date = datetime.now().strftime('%Y-%m-%d')
    
    for attempt in range(retry_count):
        try:
            ticker = yf.Ticker(symbol)
            hist = ticker.history(start=start_date, end=end_date)
            
            if hist.empty or len(hist) < 100:  # è‡³å°‘éœ€è¦ 100 å€‹äº¤æ˜“æ—¥
                return None
            
            # åªè¿”å›æ—¥æœŸå’Œæ”¶ç›¤åƒ¹
            return {
                'symbol': symbol,
                'dates': hist.index.strftime('%Y-%m-%d').tolist(),
                'close': hist['Close'].astype(float).tolist()
            }
        except Exception as e:
            if attempt < retry_count - 1:
                # ç­‰å¾…å¾Œé‡è©¦ï¼ˆæŒ‡æ•¸é€€é¿ï¼‰
                time.sleep(0.5 * (2 ** attempt))
                continue
            else:
                # æœ€å¾Œä¸€æ¬¡å˜—è©¦å¤±æ•—
                return None
    
    return None

def download_batch_with_rate_limit(symbols: List[str], start_date: str, end_date: Optional[str], 
                                   max_workers: int = 15, batch_size: int = 100) -> Dict[str, dict]:
    """åˆ†æ‰¹ä¸‹è¼‰è‚¡ç¥¨æ•¸æ“šï¼Œå¸¶é€Ÿç‡é™åˆ¶"""
    results = {}
    total = len(symbols)
    processed = 0
    
    # å°‡è‚¡ç¥¨åˆ—è¡¨åˆ†æˆå°æ‰¹æ¬¡
    for batch_start in range(0, total, batch_size):
        batch_end = min(batch_start + batch_size, total)
        batch_symbols = symbols[batch_start:batch_end]
        
        print(f"\nè™•ç†æ‰¹æ¬¡ {batch_start}-{batch_end} / {total}...")
        
        # ä¸¦è¡Œä¸‹è¼‰é€™æ‰¹è‚¡ç¥¨
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            future_to_symbol = {
                executor.submit(download_stock_close_only, symbol, start_date, end_date): symbol
                for symbol in batch_symbols
            }
            
            for future in as_completed(future_to_symbol):
                symbol = future_to_symbol[future]
                try:
                    data = future.result(timeout=30)  # 30ç§’è¶…æ™‚
                    if data:
                        results[symbol] = data
                    processed += 1
                    
                    if processed % 50 == 0:
                        print(f"å·²è™•ç†: {processed}/{total} ({processed/total*100:.1f}%)")
                        
                except Exception as e:
                    print(f"è™•ç† {symbol} å¤±æ•—: {e}")
                    processed += 1
        
        # æ‰¹æ¬¡é–“çŸ­æš«å»¶é²ï¼Œé¿å…é€Ÿç‡é™åˆ¶
        if batch_end < total:
            time.sleep(1)
    
    return results

def calculate_correlation_batch_optimized(index_data: dict, stock_symbols: List[str], 
                                         start_date: str = '2020-01-01', 
                                         end_date: Optional[str] = None,
                                         max_workers: int = 15,
                                         batch_size: int = 100) -> List[Dict]:
    """å„ªåŒ–çš„æ‰¹æ¬¡ç›¸é—œæ€§è¨ˆç®—"""
    print(f"\n{'='*60}")
    print(f"é–‹å§‹åˆ†æ‰¹ä¸‹è¼‰å’Œè¨ˆç®— {len(stock_symbols)} æ”¯è‚¡ç¥¨çš„ç›¸é—œæ€§")
    print(f"åƒæ•¸: æ‰¹æ¬¡å¤§å°={batch_size}, æœ€å¤§å·¥ä½œç·šç¨‹={max_workers}")
    print(f"{'='*60}")
    
    start_time = time.time()
    
    # æº–å‚™æŒ‡æ•¸æ•¸æ“š
    index_df = pd.DataFrame({
        'date': index_data['dates'],
        'close': index_data['close']
    })
    index_df['date'] = pd.to_datetime(index_df['date'])
    index_df = index_df.set_index('date')
    
    # ç¬¬ä¸€æ­¥ï¼šåˆ†æ‰¹ä¸‹è¼‰æ‰€æœ‰è‚¡ç¥¨æ•¸æ“š
    print("\néšæ®µ 1: ä¸‹è¼‰è‚¡ç¥¨æ•¸æ“š")
    stock_data_dict = download_batch_with_rate_limit(
        stock_symbols, start_date, end_date, max_workers, batch_size
    )
    
    download_time = time.time() - start_time
    print(f"\nä¸‹è¼‰å®Œæˆ: {len(stock_data_dict)}/{len(stock_symbols)} æ”¯è‚¡ç¥¨ (è€—æ™‚ {download_time:.1f}ç§’)")
    
    # ç¬¬äºŒæ­¥ï¼šè¨ˆç®—ç›¸é—œæ€§
    print("\néšæ®µ 2: è¨ˆç®—ç›¸é—œæ€§")
    results = []
    successful = 0
    failed = 0
    
    for symbol, stock_data in stock_data_dict.items():
        try:
            # å‰µå»º DataFrame ä¸¦å°é½Šæ—¥æœŸ
            stock_df = pd.DataFrame({
                'date': stock_data['dates'],
                'close': stock_data['close']
            })
            stock_df['date'] = pd.to_datetime(stock_df['date'])
            stock_df = stock_df.set_index('date')
            
            # åˆä½µæ•¸æ“š
            merged = index_df.join(stock_df, how='inner', rsuffix='_stock')
            
            if len(merged) < 50:  # è‡³å°‘éœ€è¦ 50 å€‹å…±åŒäº¤æ˜“æ—¥
                failed += 1
                continue
            
            # è¨ˆç®—ç›¸é—œä¿‚æ•¸
            correlation, p_value = pearsonr(
                merged['close'].values,
                merged['close_stock'].values
            )
            
            # ç²å–è‚¡ç¥¨åç¨±ï¼ˆä½¿ç”¨ç·©å­˜æ•¸æ“šï¼‰
            try:
                ticker_info = yf.Ticker(symbol)
                name = ticker_info.info.get('longName', symbol)
            except:
                name = symbol
            
            results.append({
                'symbol': symbol,
                'name': name,
                'correlation': float(correlation),
                'p_value': float(p_value),
                'data_points': len(merged)
            })
            
            successful += 1
            
            if successful % 100 == 0:
                print(f"ç›¸é—œæ€§è¨ˆç®—é€²åº¦: {successful}/{len(stock_data_dict)}")
            
        except Exception as e:
            print(f"è¨ˆç®— {symbol} ç›¸é—œæ€§å¤±æ•—: {e}")
            failed += 1
    
    total_time = time.time() - start_time
    print(f"\n{'='*60}")
    print(f"å®Œæˆ! æˆåŠŸ: {successful}, å¤±æ•—: {failed}")
    print(f"ç¸½è€—æ™‚: {total_time:.1f}ç§’ (ä¸‹è¼‰: {download_time:.1f}ç§’, è¨ˆç®—: {total_time-download_time:.1f}ç§’)")
    print(f"å¹³å‡é€Ÿåº¦: {len(stock_symbols)/total_time:.1f} è‚¡ç¥¨/ç§’")
    print(f"{'='*60}\n")
    
    # æŒ‰ç›¸é—œä¿‚æ•¸çµ•å°å€¼æ’åº
    results.sort(key=lambda x: abs(x['correlation']), reverse=True)
    
    return results

@app.route('/nasdaq/download-all', methods=['POST'])
def download_all_nasdaq_stocks():
    """ä¸‹è¼‰æ‰€æœ‰é‚£æ–¯é”å…‹è‚¡ç¥¨çš„æ­·å²è³‡æ–™ï¼ˆä¸è¨ˆç®—ç›¸é—œæ€§ï¼‰"""
    print("\n" + "="*50)
    print("API è«‹æ±‚: ä¸‹è¼‰æ‰€æœ‰é‚£æ–¯é”å…‹è‚¡ç¥¨æ­·å²è³‡æ–™")
    print("="*50)
    
    try:
        # ç²å–åƒæ•¸
        data = request.get_json() or {}
        start_date = data.get('start_date', request.args.get('start_date', '2020-01-01'))
        end_date = data.get('end_date', request.args.get('end_date', None))
        
        print(f"åƒæ•¸: start_date={start_date}, end_date={end_date}")
        
        # ç²å–æ‰€æœ‰è‚¡ç¥¨ä»£ç¢¼
        tickers = get_nasdaq_tickers()
        
        if not tickers:
            return jsonify({'error': 'ç„¡æ³•ç²å–è‚¡ç¥¨åˆ—è¡¨'}), 500
        
        print(f"å…±æœ‰ {len(tickers)} æ”¯è‚¡ç¥¨éœ€è¦ä¸‹è¼‰")
        
        # åˆ†æ‰¹ä¸‹è¼‰æ‰€æœ‰è‚¡ç¥¨æ•¸æ“š
        stock_data_dict = download_batch_with_rate_limit(
            tickers, start_date, end_date,
            max_workers=15,
            batch_size=100
        )
        
        # çµ±è¨ˆçµæœ
        successful = len(stock_data_dict)
        failed = len(tickers) - successful
        
        # çµ±è¨ˆæ•¸æ“šé»æ•¸
        total_data_points = sum(len(data['close']) for data in stock_data_dict.values())
        
        # ç”Ÿæˆæ‘˜è¦
        summary = {
            'total_tickers': len(tickers),
            'successful_downloads': successful,
            'failed_downloads': failed,
            'success_rate': f"{successful/len(tickers)*100:.1f}%",
            'total_data_points': total_data_points,
            'date_range': {
                'start': start_date,
                'end': end_date or datetime.now().strftime('%Y-%m-%d')
            },
            'downloaded_symbols': list(stock_data_dict.keys())[:50]  # åªè¿”å›å‰50å€‹ä½œç‚ºç¤ºä¾‹
        }
        
        print(f"\nä¸‹è¼‰å®Œæˆ:")
        print(f"  æˆåŠŸ: {successful}/{len(tickers)} ({summary['success_rate']})")
        print(f"  å¤±æ•—: {failed}")
        print(f"  ç¸½æ•¸æ“šé»: {total_data_points:,}")
        
        return jsonify({
            'status': 'success',
            'message': f'æˆåŠŸä¸‹è¼‰ {successful} æ”¯è‚¡ç¥¨çš„æ­·å²è³‡æ–™',
            'summary': summary
        })
        
    except Exception as e:
        print(f"éŒ¯èª¤: {e}")
        import traceback
        traceback.print_exc()
        return jsonify({'error': str(e)}), 500

@app.route('/nasdaq/download-status', methods=['GET'])
def get_download_status():
    """ç²å–å·²ç·©å­˜çš„è‚¡ç¥¨æ•¸é‡"""
    try:
        if not REDIS_AVAILABLE:
            return jsonify({
                'status': 'unavailable',
                'message': 'Redis ç·©å­˜ä¸å¯ç”¨'
            })
        
        # ç²å–æ‰€æœ‰è‚¡ç¥¨ä»£ç¢¼
        tickers = get_nasdaq_tickers()
        
        # æª¢æŸ¥æœ‰å¤šå°‘è‚¡ç¥¨å·²è¢«ç·©å­˜
        start_date = request.args.get('start_date', '2020-01-01')
        end_date = request.args.get('end_date', None)
        
        cached_count = 0
        cache_keys = []
        
        for symbol in tickers[:100]:  # åªæª¢æŸ¥å‰100å€‹ä»¥æé«˜é€Ÿåº¦
            cache_key = f"download_stock_close_only:{symbol}:{start_date}:{end_date}"
            if redis_client.exists(cache_key):
                cached_count += 1
                cache_keys.append(symbol)
        
        # ä¼°ç®—ç¸½ç·©å­˜æ•¸
        estimated_cached = int(cached_count / 100 * len(tickers))
        
        return jsonify({
            'status': 'success',
            'total_tickers': len(tickers),
            'sampled_tickers': 100,
            'cached_in_sample': cached_count,
            'estimated_total_cached': estimated_cached,
            'cache_percentage': f"{cached_count}%",
            'sample_cached_symbols': cache_keys[:20]
        })
        
    except Exception as e:
        return jsonify({'error': str(e)}), 500

@app.route('/nasdaq/all-correlation', methods=['GET'])
def get_all_nasdaq_correlation():
    """ç²å–æ‰€æœ‰é‚£æ–¯é”å…‹è‚¡ç¥¨èˆ‡æŒ‡æ•¸çš„ç›¸é—œæ€§"""
    print("\n" + "="*50)
    print("API è«‹æ±‚: è¨ˆç®—æ‰€æœ‰é‚£æ–¯é”å…‹è‚¡ç¥¨ç›¸é—œæ€§")
    print("="*50)
    
    try:
        # ç²å–åƒæ•¸
        start_date = request.args.get('start_date', '2020-01-01')
        end_date = request.args.get('end_date', None)
        limit = int(request.args.get('limit', 100))  # é»˜èªè¿”å›å‰ 100 å
        min_correlation = float(request.args.get('min_correlation', 0.5))  # æœ€å°ç›¸é—œä¿‚æ•¸
        
        print(f"åƒæ•¸: start_date={start_date}, end_date={end_date}, limit={limit}, min_correlation={min_correlation}")
        
        # ä¸‹è¼‰é‚£æ–¯é”å…‹æŒ‡æ•¸æ•¸æ“š
        print("ä¸‹è¼‰é‚£æ–¯é”å…‹æŒ‡æ•¸æ•¸æ“š...")
        index_data = download_stock_close_only('^IXIC', start_date, end_date)
        
        if index_data is None:
            return jsonify({'error': 'ç„¡æ³•ç²å–æŒ‡æ•¸æ•¸æ“š'}), 500
        
        # ç²å–æ‰€æœ‰è‚¡ç¥¨ä»£ç¢¼
        tickers = get_nasdaq_tickers()
        
        if not tickers:
            return jsonify({'error': 'ç„¡æ³•ç²å–è‚¡ç¥¨åˆ—è¡¨'}), 500
        
        print(f"å…±æœ‰ {len(tickers)} æ”¯è‚¡ç¥¨éœ€è¦åˆ†æ")
        
        # è¨ˆç®—ç›¸é—œæ€§ï¼ˆä½¿ç”¨å„ªåŒ–çš„æ‰¹æ¬¡è™•ç†å’Œç·©å­˜ï¼‰
        cache_key = f"all_correlation_v2:{start_date}:{end_date}:{len(tickers)}"
        
        if REDIS_AVAILABLE:
            try:
                cached = redis_client.get(cache_key)
                if cached:
                    print("âœ“ ä½¿ç”¨ç·©å­˜çš„ç›¸é—œæ€§çµæœ")
                    results = json.loads(gzip.decompress(cached))
                else:
                    # ä½¿ç”¨å„ªåŒ–çš„æ‰¹æ¬¡è™•ç†
                    results = calculate_correlation_batch_optimized(
                        index_data, tickers, start_date, end_date,
                        max_workers=15,  # é™ä½ä¸¦ç™¼æ•¸ä»¥æé«˜ç©©å®šæ€§
                        batch_size=100   # æ¯æ‰¹100æ”¯è‚¡ç¥¨
                    )
                    # ç·©å­˜çµæœ
                    compressed = gzip.compress(json.dumps(results).encode())
                    redis_client.setex(cache_key, CACHE_TTL_FULL_CORRELATION, compressed)
            except Exception as e:
                print(f"ç·©å­˜æ“ä½œå¤±æ•—: {e}")
                results = calculate_correlation_batch_optimized(
                    index_data, tickers, start_date, end_date,
                    max_workers=15, batch_size=100
                )
        else:
            results = calculate_correlation_batch_optimized(
                index_data, tickers, start_date, end_date,
                max_workers=15, batch_size=100
            )
        
        # éæ¿¾çµæœ
        filtered_results = [
            r for r in results 
            if abs(r['correlation']) >= min_correlation
        ]
        
        # é™åˆ¶è¿”å›æ•¸é‡
        limited_results = filtered_results[:limit]
        
        return jsonify({
            'total_analyzed': len(tickers),
            'total_with_data': len(results),
            'filtered_count': len(filtered_results),
            'returned_count': len(limited_results),
            'correlations': limited_results,
            'index': {
                'symbol': '^IXIC',
                'name': 'NASDAQ Composite',
                'data_points': len(index_data['close'])
            }
        })
        
    except Exception as e:
        print(f"éŒ¯èª¤: {e}")
        import traceback
        traceback.print_exc()
        return jsonify({'error': str(e)}), 500

@app.route('/nasdaq/tickers', methods=['GET'])
def get_tickers_list():
    """ç²å–é‚£æ–¯é”å…‹è‚¡ç¥¨åˆ—è¡¨"""
    try:
        tickers = get_nasdaq_tickers()
        return jsonify({
            'count': len(tickers),
            'tickers': tickers
        })
    except Exception as e:
        return jsonify({'error': str(e)}), 500

@app.route('/indices', methods=['GET'])
def get_all_indices():
    """ç²å–æ‰€æœ‰æ”¯æŒçš„æŒ‡æ•¸"""
    return jsonify([
        {'symbol': symbol, 'name': info['name']}
        for symbol, info in INDICES.items()
    ])

@app.route('/health', methods=['GET'])
def health_check():
    """å¥åº·æª¢æŸ¥ç«¯é»"""
    return jsonify({
        'status': 'ok',
        'message': 'API is running',
        'cache': 'enabled' if REDIS_AVAILABLE else 'disabled'
    })

@app.route('/cache/clear', methods=['POST'])
def clear_cache():
    """æ¸…é™¤æ‰€æœ‰ç·©å­˜"""
    if not REDIS_AVAILABLE:
        return jsonify({'message': 'Redis not available'})
    
    try:
        redis_client.flushdb()
        return jsonify({'message': 'ç·©å­˜å·²æ¸…é™¤'})
    except Exception as e:
        return jsonify({'error': str(e)}), 500
@app.route('/storage/download-all-to-local', methods=['POST'])
def download_all_to_local():
    """ä¸‹è¼‰æ‰€æœ‰é‚£æ–¯é”å…‹è‚¡ç¥¨æ­·å²è³‡æ–™åˆ°æœ¬åœ°å­˜å„²"""
    try:
        # ç²å–é‚£æ–¯é”å…‹è‚¡ç¥¨åˆ—è¡¨
        print("æ­£åœ¨ç²å–é‚£æ–¯é”å…‹è‚¡ç¥¨åˆ—è¡¨...")
        nasdaq_tickers = data_storage.get_nasdaq_tickers()
        
        start_date = request.json.get('start_date', '2010-01-01') if request.json else '2010-01-01'
        end_date = request.json.get('end_date', None) if request.json else None
        
        print(f"é–‹å§‹ä¸‹è¼‰ {len(nasdaq_tickers)} æ”¯è‚¡ç¥¨çš„æ­·å²è³‡æ–™ (å¾ {start_date})")
        
        # åŸ·è¡Œæ‰¹é‡ä¸‹è¼‰
        result = data_storage.bulk_download_to_local(
            symbols=nasdaq_tickers,
            start_date=start_date,
            end_date=end_date
        )
        
        return jsonify(result)
    except Exception as e:
        print(f"ä¸‹è¼‰éŒ¯èª¤: {str(e)}")
        return jsonify({'error': str(e)}), 500

@app.route('/storage/update-incremental', methods=['POST'])
def update_incremental():
    """å¢é‡æ›´æ–°æœ¬åœ°å­˜å„²çš„è‚¡ç¥¨è³‡æ–™ï¼ˆåªä¸‹è¼‰æ–°æ•¸æ“šï¼‰"""
    try:
        # ç²å–é‚£æ–¯é”å…‹è‚¡ç¥¨åˆ—è¡¨
        nasdaq_tickers = data_storage.get_nasdaq_tickers()
        
        end_date = request.json.get('end_date', None) if request.json else None
        
        print(f"é–‹å§‹å¢é‡æ›´æ–° {len(nasdaq_tickers)} æ”¯è‚¡ç¥¨")
        
        # åŸ·è¡Œæ‰¹é‡å¢é‡æ›´æ–°
        result = data_storage.bulk_update_incremental(
            symbols=nasdaq_tickers,
            end_date=end_date
        )
        
        return jsonify(result)
    except Exception as e:
        print(f"æ›´æ–°éŒ¯èª¤: {str(e)}")
        return jsonify({'error': str(e)}), 500

@app.route('/storage/stats', methods=['GET'])
def get_storage_stats():
    """ç²å–æœ¬åœ°å­˜å„²çµ±è¨ˆè³‡è¨Š"""
    try:
        stats = data_storage.get_storage_stats()
        return jsonify(stats)
    except Exception as e:
        return jsonify({'error': str(e)}), 500

@app.route('/storage/load/<symbol>', methods=['GET'])
def load_stock_data_from_local(symbol):
    """å¾æœ¬åœ°å­˜å„²è¼‰å…¥æŒ‡å®šè‚¡ç¥¨çš„æ­·å²è³‡æ–™"""
    try:
        data = data_storage.load_stock_data(symbol)
        if data is None:
            return jsonify({'error': f'æ‰¾ä¸åˆ°è‚¡ç¥¨ {symbol} çš„è³‡æ–™'}), 404
        return jsonify(data)
    except Exception as e:
        return jsonify({'error': str(e)}), 500

@app.route('/storage/stock/<symbol>', methods=['GET'])
def get_stock_from_local(symbol):
    """å¾æœ¬åœ°å­˜å„²ç²å–å–®å€‹è‚¡ç¥¨æ•¸æ“šï¼ˆæ”¯æŒå¤šå€‹æ•¸æ“šæºï¼‰"""
    try:
        start_date = request.args.get('start_date', '2010-01-01')
        end_date = request.args.get('end_date', None)
        
        print(f"å¾æœ¬åœ°ç²å–è‚¡ç¥¨æ•¸æ“š: {symbol}, æ—¥æœŸå€é–“: {start_date} è‡³ {end_date or 'ä»Šæ—¥'}")
        
        # å˜—è©¦å¾å¤šå€‹ç›®éŒ„åŠ è¼‰è‚¡ç¥¨æ•¸æ“š
        stock_data = None
        tried_dirs = []
        
        # å…ˆå˜—è©¦å¾æ‰€æœ‰å¯èƒ½çš„ç›®éŒ„åŠ è¼‰
        possible_dirs = [
            '/app/data/stocks',          # NASDAQ
            '/app/data/dow_jones_stocks', # é“ç“Šå·¥æ¥­æŒ‡æ•¸
            '/app/data/sp500_stocks'     # S&P 500
        ]
        
        for data_dir in possible_dirs:
            tried_dirs.append(data_dir)
            file_path = os.path.join(data_dir, f"{symbol}.json.gz")
            if os.path.exists(file_path):
                print(f"  âœ“ åœ¨ {data_dir} æ‰¾åˆ° {symbol}")
                try:
                    import gzip
                    with gzip.open(file_path, 'rt', encoding='utf-8') as f:
                        stock_data = json.load(f)
                    break
                except Exception as e:
                    print(f"  âœ— å¾ {file_path} åŠ è¼‰å¤±æ•—: {e}")
                    continue
        
        if not stock_data:
            print(f"  âœ— åœ¨ä»¥ä¸‹ç›®éŒ„ä¸­éƒ½æ‰¾ä¸åˆ° {symbol}: {tried_dirs}")
            return jsonify({
                'error': f'æ‰¾ä¸åˆ°è‚¡ç¥¨ {symbol} çš„æ•¸æ“š',
                'tried_dirs': tried_dirs
            }), 404
        
        # æ”¯æ´å…©ç¨®æ ¼å¼ï¼š'close' (æ–°æ ¼å¼) å’Œ 'close_prices' (èˆŠæ ¼å¼)
        close_data = stock_data.get('close') or stock_data.get('close_prices')
        if not close_data:
            return jsonify({'error': f'è‚¡ç¥¨ {symbol} æ•¸æ“šæ ¼å¼éŒ¯èª¤'}), 500
        
        # éæ¿¾æ—¥æœŸç¯„åœ
        filtered_data = []
        for i in range(len(stock_data['dates'])):
            date = stock_data['dates'][i]
            if date >= start_date and (end_date is None or date <= end_date):
                filtered_data.append({
                    'date': date,
                    'close': close_data[i]
                })
        
        if len(filtered_data) == 0:
            return jsonify({'error': 'æŒ‡å®šæ—¥æœŸç¯„åœå…§æ²’æœ‰æ•¸æ“š'}), 404
        
        return jsonify({
            'symbol': symbol,
            'name': stock_data.get('name', symbol),
            'data': filtered_data,
            'data_range': {
                'start': filtered_data[0]['date'],
                'end': filtered_data[-1]['date'],
                'trading_days': len(filtered_data)
            }
        })
    
    except Exception as e:
        print(f"ç²å–è‚¡ç¥¨æ•¸æ“šå¤±æ•—: {e}")
        import traceback
        traceback.print_exc()
        return jsonify({'error': str(e)}), 500

@app.route('/storage/correlation-analysis', methods=['POST'])
def analyze_correlation_from_local():
    """ä½¿ç”¨æœ¬åœ°å­˜å„²æ•¸æ“šåˆ†æç›¸é—œæ€§ï¼ˆåªä¿ç•™ç›¸é—œæ€§ > 0.8 çš„è‚¡ç¥¨ï¼‰"""
    try:
        # ç²å–è«‹æ±‚åƒæ•¸
        index_symbol = request.json.get('index_symbol', '^IXIC')
        threshold = request.json.get('threshold', 0.8)
        start_date = request.json.get('start_date', '2010-01-01')
        end_date = request.json.get('end_date', None)
        
        print(f"\n{'='*50}")
        print(f"æœ¬åœ°æ•¸æ“šç›¸é—œæ€§åˆ†æ")
        print(f"æŒ‡æ•¸: {INDICES.get(index_symbol, {}).get('name', index_symbol)}")
        print(f"æ—¥æœŸå€é–“: {start_date} è‡³ {end_date or 'ä»Šæ—¥'}")
        print(f"ç›¸é—œæ€§é–¾å€¼: > {threshold}")
        print(f"{'='*50}\n")
        
        # 1. å¾æœ¬åœ°å­˜å„²è¼‰å…¥æŒ‡æ•¸æ•¸æ“šï¼ˆä½¿ç”¨æŒ‡å®šçš„æ—¥æœŸå€é–“ï¼‰
        print(f"æ­£åœ¨å¾æœ¬åœ°å­˜å„²è¼‰å…¥æŒ‡æ•¸æ•¸æ“š {index_symbol}...")
        index_stock_data = data_storage.load_stock_data(index_symbol)
        
        if not index_stock_data or 'dates' not in index_stock_data:
            return jsonify({'error': 'ç„¡æ³•ç²å–æŒ‡æ•¸æ•¸æ“šï¼Œè«‹ç¢ºä¿å·²ä¸‹è¼‰åˆ°æœ¬åœ°'}), 500
        
        # æ”¯æ´å…©ç¨®æ ¼å¼ï¼š'close' (æ–°æ ¼å¼) å’Œ 'close_prices' (èˆŠæ ¼å¼)
        index_close_data = index_stock_data.get('close') or index_stock_data.get('close_prices')
        if not index_close_data:
            return jsonify({'error': 'æŒ‡æ•¸æ•¸æ“šæ ¼å¼éŒ¯èª¤'}), 500
        
        # è½‰æ›æŒ‡æ•¸æ•¸æ“šç‚ºæ—¥æœŸ-æ”¶ç›¤åƒ¹å­—å…¸ï¼Œä¸¦éæ¿¾åˆ°æŒ‡å®šæ—¥æœŸå€é–“
        index_close_dict = {}
        for i in range(len(index_stock_data['dates'])):
            date = index_stock_data['dates'][i]
            # åªä¿ç•™åœ¨æŒ‡å®šæ—¥æœŸå€é–“å…§çš„æ•¸æ“š
            if date >= start_date and (end_date is None or date <= end_date):
                index_close_dict[date] = index_close_data[i]
        
        index_dates = sorted(index_close_dict.keys())
        
        # æª¢æŸ¥æ˜¯å¦æœ‰æ•¸æ“š
        if len(index_dates) == 0:
            return jsonify({'error': f'åœ¨æŒ‡å®šçš„æ—¥æœŸå€é–“ ({start_date} ~ {end_date}) å…§æ²’æœ‰æ‰¾åˆ°æŒ‡æ•¸æ•¸æ“š'}), 400
        
        # ç¢ºä¿æ—¥æœŸåœ¨æŒ‡å®šå€é–“å…§
        index_dates_set = set(index_dates)
        
        print(f"âœ“ æŒ‡æ•¸æ•¸æ“š: {len(index_dates)} å€‹äº¤æ˜“æ—¥")
        print(f"  æ—¥æœŸç¯„åœ: {index_dates[0]} è‡³ {index_dates[-1]}\n")
        
        # 2. æ ¹æ“šæŒ‡æ•¸é¸æ“‡å°æ‡‰çš„è‚¡ç¥¨æ•¸æ“šç›®éŒ„
        print("æ­£åœ¨æƒææœ¬åœ°å­˜å„²çš„è‚¡ç¥¨...")
        stocks_dir = INDEX_DATA_DIRS.get(index_symbol, '/app/data/stocks')
        
        if not os.path.exists(stocks_dir):
            return jsonify({
                'error': f'æœ¬åœ°æ•¸æ“šç›®éŒ„ä¸å­˜åœ¨: {stocks_dir}',
                'message': f'è«‹å…ˆåŸ·è¡Œ {INDICES[index_symbol]["name"]} çš„æ•¸æ“šä¸‹è¼‰'
            }), 404
        
        stock_files = [f for f in os.listdir(stocks_dir) if f.endswith('.json.gz')]
        print(f"âœ“ å¾ {stocks_dir} æ‰¾åˆ° {len(stock_files)} æ”¯è‚¡ç¥¨\n")
        
        if len(stock_files) == 0:
            return jsonify({
                'message': 'æœ¬åœ°å­˜å„²ç‚ºç©ºï¼Œè«‹å…ˆåŸ·è¡Œåˆå§‹åŒ–ä¸‹è¼‰',
                'correlations': [],
                'total_analyzed': 0,
                'high_correlation_count': 0
            })
        
        # 3. ä¸¦è¡Œè¨ˆç®—ç›¸é—œæ€§
        print("é–‹å§‹ä¸¦è¡Œè¨ˆç®—ç›¸é—œæ€§...")
        results = []
        analyzed_count = 0
        
        def load_stock_from_dir(symbol, stocks_dir):
            """å¾æŒ‡å®šç›®éŒ„åŠ è¼‰è‚¡ç¥¨æ•¸æ“š"""
            import gzip
            file_path = os.path.join(stocks_dir, f"{symbol}.json.gz")
            try:
                with gzip.open(file_path, 'rt', encoding='utf-8') as f:
                    return json.load(f)
            except Exception as e:
                print(f"å¾ {file_path} åŠ è¼‰å¤±æ•—: {e}")
                return None
        
        def analyze_stock(stock_file):
            nonlocal analyzed_count
            symbol = stock_file.replace('.json.gz', '')
            
            # è·³éæŒ‡æ•¸æœ¬èº«
            if symbol == index_symbol:
                return None
            
            # è·³é NVDA_fixed (é‡è¤‡æ•¸æ“š)
            if symbol == 'NVDA_fixed':
                return None
            
            try:
                # å¾æŒ‡å®šç›®éŒ„åŠ è¼‰è‚¡ç¥¨æ•¸æ“š
                stock_data = load_stock_from_dir(symbol, stocks_dir)
                if not stock_data or 'dates' not in stock_data:
                    analyzed_count += 1  # è¨ˆæ•¸åŠ è¼‰å¤±æ•—çš„è‚¡ç¥¨
                    return None
                
                # æ”¯æ´å…©ç¨®æ ¼å¼ï¼š'close' (æ–°æ ¼å¼) å’Œ 'close_prices' (èˆŠæ ¼å¼)
                close_data = stock_data.get('close') or stock_data.get('close_prices')
                if not close_data:
                    analyzed_count += 1  # è¨ˆæ•¸ç„¡æ”¶ç›¤åƒ¹æ•¸æ“šçš„è‚¡ç¥¨
                    return None
                
                # å‰µå»ºè‚¡ç¥¨çš„æ—¥æœŸ-æ”¶ç›¤åƒ¹å­—å…¸ï¼Œä¸¦éæ¿¾åˆ°æŒ‡å®šæ—¥æœŸå€é–“
                stock_close_dict = {}
                for i in range(len(stock_data['dates'])):
                    date = stock_data['dates'][i]
                    # åªä¿ç•™åœ¨æŒ‡å®šæ—¥æœŸå€é–“å…§çš„æ•¸æ“š
                    if date >= start_date and (end_date is None or date <= end_date):
                        stock_close_dict[date] = close_data[i]
                
                # æ‰¾å‡ºèˆ‡æŒ‡æ•¸å…±åŒçš„äº¤æ˜“æ—¥ï¼ˆå·²ç¶“åœ¨æŒ‡å®šå€é–“å…§ï¼‰
                common_dates = sorted(index_dates_set & set(stock_close_dict.keys()))
                
                if len(common_dates) < 30:  # è‡³å°‘éœ€è¦30å€‹äº¤æ˜“æ—¥
                    analyzed_count += 1  # è¨ˆæ•¸äº¤æ˜“æ—¥ä¸è¶³çš„è‚¡ç¥¨
                    return None
                
                # æå–å…±åŒæ—¥æœŸçš„æ”¶ç›¤åƒ¹
                index_closes = [index_close_dict[date] for date in common_dates]
                stock_closes = [stock_close_dict[date] for date in common_dates]
                
                # è¨ˆç®—ç›¸é—œæ€§
                correlation, p_value = pearsonr(index_closes, stock_closes)
                
                analyzed_count += 1
                if analyzed_count % 100 == 0:
                    print(f"å·²åˆ†æ {analyzed_count}/{len(stock_files)} æ”¯è‚¡ç¥¨...")
                
                # åªè¿”å›ç›¸é—œæ€§å¤§æ–¼é–¾å€¼çš„è‚¡ç¥¨
                if correlation > threshold:
                    # ç²å–è‚¡ç¥¨åç¨±
                    try:
                        ticker = yf.Ticker(symbol)
                        name = ticker.info.get('longName', symbol)
                    except:
                        name = symbol
                    
                    return {
                        'symbol': symbol,
                        'name': name,
                        'correlation': float(correlation),
                        'data_points': len(common_dates)
                    }
                
                return None
                
            except Exception as e:
                print(f"åˆ†æ {symbol} å¤±æ•—: {e}")
                return None
        
        # ä½¿ç”¨ç·šç¨‹æ± ä¸¦è¡Œè™•ç†
        with ThreadPoolExecutor(max_workers=20) as executor:
            futures = [executor.submit(analyze_stock, stock_file) for stock_file in stock_files]
            
            for future in as_completed(futures):
                result = future.result()
                if result is not None:
                    results.append(result)
        
        # æŒ‰ç›¸é—œæ€§æ’åº
        results.sort(key=lambda x: x['correlation'], reverse=True)
        
        print(f"\n{'='*50}")
        print(f"ç›¸é—œæ€§åˆ†æå®Œæˆï¼")
        print(f"æ—¥æœŸå€é–“: {start_date} è‡³ {end_date or 'ä»Šæ—¥'}")
        print(f"ç¸½åˆ†æè‚¡ç¥¨æ•¸: {analyzed_count}")
        print(f"é«˜ç›¸é—œæ€§è‚¡ç¥¨æ•¸ (>{threshold}): {len(results)}")
        print(f"{'='*50}\n")
        
        return jsonify({
            'correlations': results,
            'total_analyzed': analyzed_count,
            'high_correlation_count': len(results),
            'threshold': threshold,
            'index_symbol': index_symbol,
            'index_name': INDICES.get(index_symbol, {}).get('name', index_symbol),
            'start_date': start_date,
            'end_date': end_date or datetime.now().strftime('%Y-%m-%d'),
            'date_range': {
                'start': index_dates[0] if index_dates else start_date,
                'end': index_dates[-1] if index_dates else end_date,
                'trading_days': len(index_dates)
            }
        })
        
    except Exception as e:
        print(f"ç›¸é—œæ€§åˆ†æéŒ¯èª¤: {str(e)}")
        import traceback
        traceback.print_exc()
        return jsonify({'error': str(e)}), 500

# ===== é“ç“Šå·¥æ¥­æŒ‡æ•¸å°ˆç”¨ç«¯é» =====

@app.route('/dow-jones/download-all', methods=['POST'])
def download_dow_jones_stocks():
    """ä¸‹è¼‰é“ç“Šå·¥æ¥­æŒ‡æ•¸30æ”¯æˆåˆ†è‚¡çš„æ­·å²è³‡æ–™åˆ°æœ¬åœ°å­˜å„²"""
    try:
        import dow_jones_downloader
        
        start_date = request.json.get('start_date', '2010-01-01') if request.json else '2010-01-01'
        end_date = request.json.get('end_date', None) if request.json else None
        max_workers = request.json.get('max_workers', 5) if request.json else 5
        
        print(f"\né–‹å§‹ä¸‹è¼‰é“ç“Šå·¥æ¥­æŒ‡æ•¸æˆåˆ†è‚¡æ­·å²è³‡æ–™")
        print(f"èµ·å§‹æ—¥æœŸ: {start_date}")
        print(f"çµæŸæ—¥æœŸ: {end_date or 'ä»Šå¤©'}")
        print(f"ä¸¦è¡Œç·šç¨‹: {max_workers}")
        
        # åŸ·è¡Œæ‰¹é‡ä¸‹è¼‰
        result = dow_jones_downloader.bulk_download_dow_jones(
            start_date=start_date,
            end_date=end_date,
            max_workers=max_workers
        )
        
        return jsonify({
            'success': True,
            'message': f'æˆåŠŸä¸‹è¼‰ {result["successful"]}/{result["total_stocks"]} æ”¯è‚¡ç¥¨',
            'total_stocks': result['total_stocks'],
            'successful': result['successful'],
            'failed': result['failed'],
            'elapsed_time_seconds': result['elapsed_time_seconds'],
            'data_dir': '/app/data/dow_jones_stocks'
        })
        
    except Exception as e:
        print(f"ä¸‹è¼‰é“ç“Šå·¥æ¥­æŒ‡æ•¸è‚¡ç¥¨å¤±æ•—: {str(e)}")
        import traceback
        traceback.print_exc()
        return jsonify({'error': str(e)}), 500

@app.route('/dow-jones/download-status', methods=['GET'])
def get_dow_jones_download_status():
    """ç²å–é“ç“Šå·¥æ¥­æŒ‡æ•¸æ•¸æ“šä¸‹è¼‰ç‹€æ…‹"""
    try:
        import os
        import json
        
        data_dir = '/app/data/dow_jones_stocks'
        meta_file = '/app/data/dow_jones_meta.json'
        
        # æª¢æŸ¥æ•¸æ“šç›®éŒ„
        if not os.path.exists(data_dir):
            return jsonify({
                'downloaded': False,
                'message': 'å°šæœªä¸‹è¼‰é“ç“Šå·¥æ¥­æŒ‡æ•¸æˆåˆ†è‚¡æ•¸æ“š'
            })
        
        # çµ±è¨ˆå·²ä¸‹è¼‰çš„è‚¡ç¥¨æ•¸é‡
        stock_files = [f for f in os.listdir(data_dir) if f.endswith('.json.gz')]
        
        # è®€å–å…ƒæ•¸æ“š
        meta = None
        if os.path.exists(meta_file):
            with open(meta_file, 'r', encoding='utf-8') as f:
                meta = json.load(f)
        
        return jsonify({
            'downloaded': True,
            'total_files': len(stock_files),
            'data_dir': data_dir,
            'meta': meta
        })
        
    except Exception as e:
        return jsonify({'error': str(e)}), 500

# ===== S&P 500 å°ˆç”¨ç«¯é» =====

@app.route('/sp500/download-all', methods=['POST'])
def download_sp500_stocks():
    """ä¸‹è¼‰ S&P 500 æˆåˆ†è‚¡çš„æ­·å²è³‡æ–™åˆ°æœ¬åœ°å­˜å„²"""
    try:
        import sp500_downloader
        
        start_date = request.json.get('start_date', '2010-01-01') if request.json else '2010-01-01'
        end_date = request.json.get('end_date', None) if request.json else None
        max_workers = request.json.get('max_workers', 10) if request.json else 10
        
        print(f"\né–‹å§‹ä¸‹è¼‰ S&P 500 æˆåˆ†è‚¡æ­·å²è³‡æ–™")
        print(f"èµ·å§‹æ—¥æœŸ: {start_date}")
        print(f"çµæŸæ—¥æœŸ: {end_date or 'ä»Šå¤©'}")
        print(f"ä¸¦è¡Œç·šç¨‹: {max_workers}")
        
        # åŸ·è¡Œæ‰¹é‡ä¸‹è¼‰
        result = sp500_downloader.bulk_download_sp500(
            start_date=start_date,
            end_date=end_date,
            max_workers=max_workers
        )
        
        if not result:
            return jsonify({'error': 'ä¸‹è¼‰å¤±æ•—'}), 500
        
        return jsonify({
            'success': True,
            'message': f'æˆåŠŸä¸‹è¼‰ {result["successful"]}/{result["total_stocks"]} æ”¯è‚¡ç¥¨',
            'total_stocks': result['total_stocks'],
            'successful': result['successful'],
            'failed': result['failed'],
            'success_rate': round(result['successful']/result['total_stocks']*100, 1),
            'elapsed_time_seconds': result['elapsed_time_seconds'],
            'data_dir': '/app/data/sp500_stocks'
        })
        
    except Exception as e:
        print(f"ä¸‹è¼‰ S&P 500 è‚¡ç¥¨å¤±æ•—: {str(e)}")
        import traceback
        traceback.print_exc()
        return jsonify({'error': str(e)}), 500

@app.route('/sp500/download-status', methods=['GET'])
def get_sp500_download_status():
    """ç²å– S&P 500 æ•¸æ“šä¸‹è¼‰ç‹€æ…‹"""
    try:
        import os
        import json
        
        data_dir = '/app/data/sp500_stocks'
        meta_file = '/app/data/sp500_meta.json'
        
        # æª¢æŸ¥æ•¸æ“šç›®éŒ„
        if not os.path.exists(data_dir):
            return jsonify({
                'downloaded': False,
                'message': 'å°šæœªä¸‹è¼‰ S&P 500 æˆåˆ†è‚¡æ•¸æ“š'
            })
        
        # çµ±è¨ˆå·²ä¸‹è¼‰çš„è‚¡ç¥¨æ•¸é‡
        stock_files = [f for f in os.listdir(data_dir) if f.endswith('.json.gz')]
        
        # è®€å–å…ƒæ•¸æ“š
        meta = None
        if os.path.exists(meta_file):
            with open(meta_file, 'r', encoding='utf-8') as f:
                meta = json.load(f)
        
        return jsonify({
            'downloaded': True,
            'total_files': len(stock_files),
            'data_dir': data_dir,
            'meta': meta
        })
        
    except Exception as e:
        return jsonify({'error': str(e)}), 500

@app.route('/storage/drawdown-periods', methods=['POST'])
def get_drawdown_periods():
    """
    è¨ˆç®—ä¸¦è¿”å›æŒ‡æ•¸çš„æ³¢æ®µä¸‹è·Œå€é–“
    ç•¶è·Œå¹…è¶…éæŒ‡å®šé–¾å€¼æ™‚æ¨™è¨˜ç‚ºé‡è¦ä¸‹è·Œå€é–“
    """
    try:
        data = request.get_json()
        index_symbol = data.get('index_symbol', '^IXIC')
        threshold = float(data.get('threshold', 0.15))  # é»˜èª15%
        
        print(f"\nè¨ˆç®—æ³¢æ®µä¸‹è·Œå€é–“: {index_symbol}, é–¾å€¼: {threshold*100}%")
        
        # å¾æœ¬åœ°å­˜å„²åŠ è¼‰æŒ‡æ•¸æ•¸æ“š
        stock_data = data_storage.load_stock_data(index_symbol)
        
        # å¦‚æœæœ¬åœ°æ²’æœ‰ï¼Œå˜—è©¦å¾ yfinance ç²å–
        if not stock_data:
            print(f"æœ¬åœ°æ²’æœ‰ {index_symbol} æ•¸æ“šï¼Œå˜—è©¦å¾ yfinance ç²å–...")
            from datetime import datetime, timedelta
            end_date = datetime.now()
            start_date = end_date - timedelta(days=365*16)  # ç²å–16å¹´æ•¸æ“š
            
            ticker = yf.Ticker(index_symbol)
            hist = ticker.history(start=start_date, end=end_date)
            
            if hist.empty:
                return jsonify({'error': f'ç„¡æ³•ç²å– {index_symbol} çš„æ•¸æ“š'}), 404
            
            # è½‰æ›ç‚ºæ¨™æº–æ ¼å¼
            stock_data = {
                'symbol': index_symbol,
                'data': [
                    {
                        'date': date.strftime('%Y-%m-%d'),
                        'close': float(row['Close'])
                    }
                    for date, row in hist.iterrows()
                ]
            }
            print(f"æˆåŠŸå¾ yfinance ç²å– {len(stock_data['data'])} ç­†æ•¸æ“š")
        
        # è½‰æ›ç‚ºDataFrame - å…¼å®¹å…©ç¨®æ ¼å¼
        if 'data' in stock_data and stock_data['data']:
            # æ–°æ ¼å¼: {data: [{date, close}, ...]}
            df = pd.DataFrame(stock_data['data'])
        elif 'dates' in stock_data and 'close' in stock_data:
            # èˆŠæ ¼å¼: {dates: [...], close: [...]}
            df = pd.DataFrame({
                'date': stock_data['dates'],
                'close': stock_data['close']
            })
        else:
            return jsonify({'error': f'{index_symbol} æ•¸æ“šæ ¼å¼éŒ¯èª¤'}), 400
        
        df['date'] = pd.to_datetime(df['date'])
        df = df.sort_values('date')
        df['close'] = df['close'].astype(float)
        
        # è¨ˆç®—æ³¢æ®µé«˜é»å’Œä¸‹è·Œå€é–“ï¼ˆå„ªåŒ–ç®—æ³•ï¼šåªè¿½è¸ªé‡è¦çš„ä¸‹è·Œï¼‰
        drawdown_periods = []
        
        # æ–¹æ³•ï¼šè¿½è¸ªrunning maximumï¼Œç•¶å¾é«˜é»ä¸‹è·Œè¶…éé–¾å€¼æ™‚æ¨™è¨˜
        # ä½†å…è¨±åœ¨å‰µæ–°é«˜ä¹‹å‰å°±æ¨™è¨˜ä¸‹è·Œå€é–“ï¼ˆä¸è¦æ±‚å¿…é ˆæ¢å¾©åˆ°å³°å€¼ï¼‰
        running_max = df['close'].iloc[0]
        running_max_date = df['date'].iloc[0]
        running_max_idx = 0
        
        for idx in range(len(df)):
            current_price = df['close'].iloc[idx]
            current_date = df['date'].iloc[idx]
            
            # å¦‚æœå‰µæ–°é«˜ï¼Œæ›´æ–°running maximum
            if current_price > running_max:
                running_max = current_price
                running_max_date = current_date
                running_max_idx = idx
            else:
                # è¨ˆç®—å¾running maxçš„è·Œå¹…
                drawdown_pct = (running_max - current_price) / running_max
                
                # å¦‚æœè·Œå¹…è¶…éé–¾å€¼ï¼Œæª¢æŸ¥æ˜¯å¦æ‡‰è©²å‰µå»ºæ–°å€é–“
                if drawdown_pct >= threshold:
                    # å¾running_maxåˆ°ç•¶å‰æ‰¾æœ€ä½é»
                    segment = df.iloc[running_max_idx:idx+1]
                    trough_idx = segment['close'].idxmin()
                    trough_price = segment.loc[trough_idx, 'close']
                    trough_date = segment.loc[trough_idx, 'date']
                    
                    actual_drawdown = (running_max - trough_price) / running_max
                    
                    # æª¢æŸ¥æ˜¯å¦å·²ç¶“è¨˜éŒ„éé€™å€‹å³°å€¼çš„ä¸‹è·Œ
                    already_recorded = False
                    for existing in drawdown_periods:
                        if existing['peak_date'] == running_max_date.strftime('%Y-%m-%d'):
                            # å¦‚æœæ–°çš„è°·åº•æ›´ä½ï¼Œæ›´æ–°è¨˜éŒ„
                            if trough_price < existing['trough_price']:
                                existing['trough_date'] = trough_date.strftime('%Y-%m-%d')
                                existing['trough_price'] = float(trough_price)
                                existing['drawdown_pct'] = float(actual_drawdown)
                                existing['duration_days'] = int((trough_date - running_max_date).days)
                            already_recorded = True
                            break
                    
                    # å¦‚æœé‚„æ²’è¨˜éŒ„ï¼Œå‰µå»ºæ–°è¨˜éŒ„
                    if not already_recorded:
                        # å˜—è©¦æ‰¾æ¢å¾©æ—¥æœŸ
                        future_data = df.iloc[idx+1:]
                        recovery_date = None
                        recovery_price = None
                        
                        for future_idx in future_data.index:
                            if df.loc[future_idx, 'close'] >= running_max:
                                recovery_date = df.loc[future_idx, 'date']
                                recovery_price = df.loc[future_idx, 'close']
                                break
                        
                        drawdown_periods.append({
                            'peak_date': running_max_date.strftime('%Y-%m-%d'),
                            'peak_price': float(running_max),
                            'trough_date': trough_date.strftime('%Y-%m-%d'),
                            'trough_price': float(trough_price),
                            'drawdown_pct': float(actual_drawdown),
                            'recovery_date': recovery_date.strftime('%Y-%m-%d') if recovery_date else None,
                            'recovery_price': float(recovery_price) if recovery_price else None,
                            'duration_days': int((trough_date - running_max_date).days)
                        })
        
        # æŒ‰å³°å€¼æ—¥æœŸæ’åº
        drawdown_periods.sort(key=lambda x: x['peak_date'])
        
        print(f"æ‰¾åˆ° {len(drawdown_periods)} å€‹è¶…é {threshold*100}% çš„ä¸‹è·Œå€é–“")
        
        return jsonify({
            'drawdown_periods': drawdown_periods,
            'total_periods': len(drawdown_periods),
            'threshold': threshold,
            'index_symbol': index_symbol
        })
        
    except Exception as e:
        print(f"è¨ˆç®—æ³¢æ®µä¸‹è·ŒéŒ¯èª¤: {e}")
        import traceback
        traceback.print_exc()
        return jsonify({'error': str(e)}), 500

def startup_update_data():
    """å•Ÿå‹•æ™‚è‡ªå‹•æ›´æ–°æ•¸æ“šåˆ°æœ€æ–°"""
    try:
        print("\n" + "=" * 50)
        print("æ­£åœ¨æª¢æŸ¥ä¸¦æ›´æ–°æ•¸æ“šåˆ°æœ€æ–°...")
        print("=" * 50)
        
        # ç²å–æ‰€æœ‰é‚£æ–¯é”å…‹è‚¡ç¥¨ä»£ç¢¼
        nasdaq_tickers = data_storage.get_nasdaq_tickers()
        
        # ç²å–çµ±è¨ˆè³‡è¨Šï¼Œç¢ºèªæ˜¯å¦éœ€è¦æ›´æ–°
        stats = data_storage.get_storage_stats()
        
        if stats.get('total_stocks', 0) > 0:
            print(f"æœ¬åœ°å·²æœ‰ {stats['total_stocks']} æ”¯è‚¡ç¥¨æ•¸æ“š")
            print("åŸ·è¡Œå¢é‡æ›´æ–°ï¼Œåªä¸‹è¼‰æœ€æ–°æ•¸æ“š...")
            
            # åŸ·è¡Œå¢é‡æ›´æ–°
            result = data_storage.bulk_update_incremental(
                symbols=nasdaq_tickers,
                end_date=None  # None è¡¨ç¤ºæ›´æ–°åˆ°ä»Šå¤©
            )
            
            print(f"âœ“ æ›´æ–°å®Œæˆï¼æ›´æ–°äº† {result.get('updated', 0)} æ”¯è‚¡ç¥¨")
        else:
            print("æœ¬åœ°ç„¡æ•¸æ“šï¼Œå°‡ä¸‹è¼‰æ‰€æœ‰æ­·å²æ•¸æ“š...")
            print("é€™å¯èƒ½éœ€è¦å¹¾åˆ†é˜ï¼Œè«‹ç¨å€™...")
            
            # åŸ·è¡Œå®Œæ•´ä¸‹è¼‰
            result = data_storage.bulk_download_to_local(
                symbols=nasdaq_tickers
            )
            
            print(f"âœ“ ä¸‹è¼‰å®Œæˆï¼å…± {result.get('successful', 0)} æ”¯è‚¡ç¥¨")
        
        print("=" * 50 + "\n")
        
    except Exception as e:
        print(f"âš  æ•¸æ“šæ›´æ–°è­¦å‘Š: {str(e)}")
        print("API å°‡ä½¿ç”¨ç¾æœ‰æ•¸æ“šç¹¼çºŒé‹è¡Œ")
        print("=" * 50 + "\n")

if __name__ == '__main__':
    print("=" * 50)
    print("ç¾åœ‹è‚¡å¸‚åˆ†æç³»çµ± - å¾Œç«¯ API (å„ªåŒ–ç‰ˆ)")
    print("=" * 50)
    print("æ”¯æŒçš„æŒ‡æ•¸:")
    for symbol, info in INDICES.items():
        print(f"  - {info['name']} ({symbol})")
    print("=" * 50)
    print("å„ªåŒ–åŠŸèƒ½:")
    print("  âœ“ Redis ç·©å­˜")
    print("  âœ“ ä¸¦è¡Œæ•¸æ“šä¸‹è¼‰")
    print("  âœ“ gzip å£“ç¸®")
    print("  âœ“ å‘é‡åŒ–æ•¸æ“šè™•ç†")
    print("  âœ“ å•Ÿå‹•æ™‚è‡ªå‹•æ›´æ–°æ•¸æ“š")
    print("=" * 50)
    print("æ•¸æ“šç¯„åœ: 2010-01-01 è‡³ä»Š")
    print("=" * 50)
    print("API å•Ÿå‹•æ–¼ http://localhost:8000")
    print("=" * 50)
    
    # å•Ÿå‹•æ™‚æ›´æ–°æ•¸æ“š
    startup_update_data()
    
    app.run(host='0.0.0.0', port=8000, debug=False)
